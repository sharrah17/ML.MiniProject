# Gradient Descent 

Gradient descent is an optimization algorithm used to find the values of coefficients in a function (f) that minimizes a cost function. A cost function is a function that measures the performance of a machine learning model. It quantifies the error between true and predicted values. The goal is to minimize the cost function to reduce the errors and improve model performance. Gradient descent essentially trains a ML model.
